#!/usr/bin/perl -w

# -----------------------------------------------------------------
# Name:      mklex5c.pl
# Author:    Kiduk Yang, 4/29/2008
#            modified, 6/24/2008
#              - uses 2006 & 2007 queries
#             $Id: mklex5c.pl,v 1.1 2008/06/27 00:42:36 kiyang Exp $
# -----------------------------------------------------------------
# Description:  compute opinion probabilities of terms
#   1. op_P = p(t/opD) * p (!t/nopD)
#             p(t/opD) prob that term will occur in opinion doc 
#              = df w/ t in opD / #opD
#             p(!t/nopD) prob that term will occur in opinion doc
#              = df w/o t in nopD / #nopD
#   Note: compute P for all proximity terms instead of lexicon terms as in mklex5
# -----------------------------------------------------------------
# Argument:  arg1= 1 to run
# Input:     
#   $ddir/trainData/dnref_op - docID to blogID mapping (opinion blog)
#   $ddir/trainData/dnref_op2 - docID to blogID mapping (opinion blog)
#   $ddir/trainData/dnref_nop - docID to blogID mapping (non-opinion blog)
#   $ddir/trainData/dnref_nop2 - docID to blogID mapping (non-opinion blog)
#       DOCN TREC_DOCN REL (per line)
#   $ddir/trainData/op/docx/$docn - processed opinion blog file (after NR & sentence break)
#   $ddir/trainData/nop/docx/$docn - processed non-opinion blog file (after NR & sentence break)
#   $ddir/trainData/op2/docx/$docn - processed opinion blog file (after NR & sentence break)
#   $ddir/trainData/nop2/docx/$docn - processed non-opinion blog file (after NR & sentence break)
#   $tpdir/stoplist2   -- document stoplist
#   $tpdir/dict.arv    -- adjective, adverb, verb list
# Output:   
#   $ddir/IUb.terms                   - IU lexicon
#   $ddir/HFb.terms                   - HF lexicon
#   $ddir/HFb.arv                     - HF adjective, verb, adverbs
#   $prog        -- program     (optional)
#   $prog.log    -- program log (optional)
# -----------------------------------------------------------------
# NOTE: uses clean training data generated by mklex2.pl
# -----------------------------------------------------------------

use strict;
use Data::Dumper;
$Data::Dumper::Purity=1;

my ($debug,$filemode,$filemode2,$dirmode,$dirmode2,$author,$group);
my ($log,$logd,$sfx,$noargp,$append,@start_time);

$log=1;                              # program log flag
$debug=0;                            # debug flag
$filemode= 0640;                     # to use w/ perl chmod
$filemode2= 640;                     # to use w/ system chmod
$dirmode= 0750;                      # to use w/ perl mkdir
$dirmode2= 750;                      # to use w/ system chmod (directory)
$group= "trec";                      # group ownership of files
$author= "kiyang\@indiana.edu";      # author's email


#------------------------
# global variables
#------------------------

my $wpdir=  "/u0/widit/prog";           # widit program directory
my $tpdir=  "$wpdir/trec08";            # TREC program directory
my $pdir=   "$tpdir/blog";              # Blog track program directory
my $ddir=   "/u3/trec/blog07";          # TREC index directory
my $ddir2=   "/u3/trec/blog08";          # TREC index directory
my $qdir=   "$ddir2/query";              # query directory

my $qryTdir=   "$qdir/train";        # processed query directory
my $qryEdir=   "$qdir/test";        # processed query directory
my $qryTdir2= "$qryTdir/s0x";         # web-expanded query phrases
my $qryEdir2= "$qryEdir/s0x";         # web-expanded query phrases

my $qrelTf=  "/u1/trec/qrels/qrels.blog06";
my $qrelEf=  "/u1/trec/qrels/07.qrels.opinion";

my $stopfile= "$tpdir/stoplist1";       # stopword list
my $arvlist=  "$tpdir/dict.arv";        # adjective, adverb, verb

my $odir= "$ddir/lex2";
my $opdir= "$ddir/trainData/op/docx";    # clean opinion training data
my $nopdir= "$ddir/trainData/nop/docx";  # non-opinion training data
my $opdir2= "$ddir/trainData/op2/docx";    # clean opinion training data
my $nopdir2= "$ddir/trainData/nop2/docx";  # non-opinion training data

my $dnref_op= "$ddir/trainData/dnref_op";
my $dnref_nop= "$ddir/trainData/dnref_nop";
my $dnref_op2= "$ddir/trainData/dnref_op2";
my $dnref_nop2= "$ddir/trainData/dnref_nop2";

# output files
my $iuout= "$odir/IUb.terms";                  # IU lexicon file
my $hfout= "$odir/HFb.terms";                  # HF lexicon file
my $hfout2= "$odir/HFb.arv";                  # HF lexicon file

require "$wpdir/logsub2.pl";   # subroutine library
require "$pdir/blogsub.pl";   # blog subroutine library


#------------------------
# program arguments
#------------------------
my $prompt=
"arg1= 1 to run\n";

my %valid_args= (
0 => " 1 ",
);

my ($arg_err,$arg1)= chkargs($prompt,\%valid_args,1);
die "$arg_err\n" if ($arg_err);


#-------------------------------------------------
# start program log
#-------------------------------------------------

$sfx= "";              # program log file suffix
$noargp=1;             # if 1, do not print arguments to log
$append=0;             # log append flag

if ($log) {
    @start_time= &begin_log($odir,$filemode,$sfx,$noargp,$append);
    print LOG "INF = $opdir/*\n",
              "      $nopdir/*\n",
              "      $opdir2/*\n",
              "      $nopdir2/*\n",
              "OUTD= $odir/\n\n";
}


#-------------------------------------------------
# create word hashes : key=word, val=1  
#  - %stoplist: stopwords
#  - %abbrlist: word list for acronyms and abbreviations
#-------------------------------------------------

# create hash of adjective, adverb, & verb
my %arvlist;
&mkhash($arvlist,\%arvlist);

# create the stopword hash
my %stoplist;
&mkhash($stopfile,\%stoplist);


#-------------------------------------------------
# create hash of query terms
#-------------------------------------------------
# %qterm:  
#   k(QN -> term) = v(freq)
#-------------------------------------------------

my %qterm;
&mkQRYhash($qryTdir,$qryTdir2,\%qterm);
&mkQRYhash($qryEdir,$qryEdir2,\%qterm);

if ($debug>1) {
    foreach my $qn(sort keys %qterm) {
        print "QN=$qn\n";
        foreach my $k(sort keys %{$qterm{$qn}}) { print "   $k: $qterm{$qn}{$k}\n"; }
        print "\n";
    }
}


#-------------------------------------------------
# read in qrels file to create %qrels
#   k=blogID, v=array of QNs
#-------------------------------------------------
open(IN,$qrelTf) || die "can't read $qrelTf";
my @lines=<IN>;
close IN;
chomp @lines;
    
my (%qrel);
foreach (@lines) {
    my($qn,$dummy,$id,$rel)=split/ +/;
    next if ($rel<1);
    push(@{$qrel{$id}},$qn);
}


open(IN,$qrelEf) || die "can't read $qrelEf";
@lines=<IN>;
close IN;
chomp @lines;

foreach (@lines) {
    my($qn,$dummy,$id,$rel)=split/ +/;
    next if ($rel<1);
    push(@{$qrel{$id}},$qn);
}


#-------------------------------------------------
# create %dnref
#   - k=blogID, v=DN
#-------------------------------------------------
open(IN,$dnref_op) || die "can't read $dnref_op";
@lines=<IN>;
close IN;
chomp @lines;

my ($opn,%dnref_op)=(0);
foreach (@lines) {
    my($dn,$id,$rel)=split/ /;
    $dnref_op{$dn}=$id;
    $opn++; 
}

open(IN,$dnref_op2) || die "can't read $dnref_op2";
@lines=<IN>;
close IN;
chomp @lines;

my ($opn2,%dnref_op2)=(0);
foreach (@lines) {
    my($dn,$id,$rel)=split/ /;
    $dnref_op2{$dn}=$id;
    $opn2++;
}


open(IN,$dnref_nop) || die "can't read $dnref_nop";
@lines=<IN>;
close IN;
chomp @lines;

my ($nopn,%dnref_nop)=(0);
foreach (@lines) {
    my($dn,$id,$rel)=split/ /;
    $dnref_nop{$dn}=$id;
    $nopn++;
}


open(IN,$dnref_nop2) || die "can't read $dnref_nop2";
@lines=<IN>;
close IN;
chomp @lines;

my ($nopn2,%dnref_nop2)=(0);
foreach (@lines) {
    my($dn,$id,$rel)=split/ /;
    $dnref_nop2{$dn}=$id;
    $nopn2++;
}


#-------------------------------------------------
# 1. process non-opinion training data
# 2. compute term frequencies
#    - { term => op|nop => df }
#-------------------------------------------------

my (%HF,%IU);

my $opcnt=0;
for(my $i=1; $i<=$opn; $i++) {
    my $inf="$opdir/$i";

    my %qwd;
    foreach my $qn(@{$qrel{$dnref_op{$i}}}) {
        foreach my $wd(keys %{$qterm{$qn}}) {
            $qwd{$wd}++;
        }
    }

    open(IN,$inf) || die "can't read $inf";
    my @lines=<IN>;
    close IN;

    my %goodL;  # good line numbers
    for(my $i=0; $i<@lines; $i++) {
        my $found=0;
        foreach my $wd(keys %qwd) {
            if ($lines[$i]=~/\b$wd\b/i) { 
                $found=1;
                last;
            }
        }
        if ($found) { 
            for(my $k=$i-2;$k<=$i+2;$k++) { 
                last if ($k<0 || $k>=@lines); 
                $goodL{$k}++;
            }
            $i+=2;
        }
    }

    # treat each sentence as document
    foreach my $k(keys %goodL) {
        &cntDF($lines[$k],'op');
        $opcnt++;
    }

}

for(my $i=1; $i<=$opn2; $i++) {
    my $inf="$opdir2/$i";

    my %qwd;
    foreach my $qn(@{$qrel{$dnref_op2{$i}}}) {
        foreach my $wd(keys %{$qterm{$qn}}) {
            $qwd{$wd}++;
        }
    }

    open(IN,$inf) || die "can't read $inf";
    my @lines=<IN>;
    close IN;

    my %goodL;  # good line numbers
    for(my $i=0; $i<@lines; $i++) {
        my $found=0;
        foreach my $wd(keys %qwd) {
            if ($lines[$i]=~/\b$wd\b/i) { 
                $found=1;
                last;
            }
        }
        if ($found) { 
            for(my $k=$i-2;$k<=$i+2;$k++) { 
                last if ($k<0 || $k>=@lines); 
                $goodL{$k}++;
            }
            $i+=2;
        }
    }

    # treat each sentence as document
    foreach my $k(keys %goodL) {
        &cntDF($lines[$k],'op');
        $opcnt++;
    }

}


#-------------------------------------------------
# 1. process non-opinion training data
# 2. compute term frequencies
#    - { term => op|nop => df }
#-------------------------------------------------

my $nopcnt=0;
for(my $i=1; $i<=$nopn; $i++) {
    my $inf="$nopdir/$i";

    my %qwd;
    foreach my $qn(@{$qrel{$dnref_nop{$i}}}) {
        foreach my $wd(keys %{$qterm{$qn}}) {
            $qwd{$wd}++;
        }
    }


    open(IN,$inf) || die "can't read $inf";
    my @lines=<IN>;
    close IN;

    my %goodL;  # good line numbers
    for(my $i=0; $i<@lines; $i++) {
        my $found=0;
        foreach my $wd(keys %qwd) {
            if ($lines[$i]=~/\b$wd\b/i) { 
                $found=1;
                last;
            }
        }
        if ($found) { 
            for(my $k=$i-2;$k<=$i+2;$k++) { 
                last if ($k<0 || $k>=@lines); 
                $goodL{$k}++;
            }
            $i+=2;
        }
    }

    # treat each sentence as document
    foreach my $k(keys %goodL) {
        &cntDF($lines[$k],'nop');
        $nopcnt++;
    }

}

for(my $i=1; $i<=$nopn2; $i++) {
    my $inf="$nopdir2/$i";

    my %qwd;
    foreach my $qn(@{$qrel{$dnref_nop2{$i}}}) {
        foreach my $wd(keys %{$qterm{$qn}}) {
            $qwd{$wd}++;
        }
    }


    open(IN,$inf) || die "can't read $inf";
    my @lines=<IN>;
    close IN;

    my %goodL;  # good line numbers
    for(my $i=0; $i<@lines; $i++) {
        my $found=0;
        foreach my $wd(keys %qwd) {
            if ($lines[$i]=~/\b$wd\b/i) { 
                $found=1;
                last;
            }
        }
        if ($found) { 
            for(my $k=$i-2;$k<=$i+2;$k++) { 
                last if ($k<0 || $k>=@lines); 
                $goodL{$k}++;
            }
            $i+=2;
        }
    }

    # treat each sentence as document
    foreach my $k(keys %goodL) {
        &cntDF($lines[$k],'nop');
        $nopcnt++;
    }

}

print LOG "Documents: $opn Positive, $nopn Negative\n",
          "Sentences: $opcnt Positive, $nopcnt Negative\n\n";

&mkLex($hfout,\%HF,$hfout2,\%arvlist);
&mkLex($iuout,\%IU);



#-------------------------------------------------
# end program
#-------------------------------------------------

&end_log($pdir,$odir,$filemode,@start_time) if ($log);

# notify author of program completion
#&notify($sfx,$author);


##############################################
# SUBROUTINES
##############################################

BEGIN { print STDOUT "\n"; }
END { print STDOUT "\n"; }

#-----------------------------------------------------------
#  create hash from file
#-----------------------------------------------------------
#  arg1 = infile
#  arg2 = pointer to hash to create
#-----------------------------------------------------------
sub mkhash {
    my($file,$hp)=@_;

    open(IN,$file) || die "can't read $file";
    my @terms=<IN>;
    close IN;
    chomp @terms;
    foreach my $word(@terms) { $$hp{$word}=1; }

} #endsub mkhash


#-------------------------------------------------
# count DF
#-------------------------------------------------
#  arg1 = text string
#  arg2 = DF type (op, nop)
#-------------------------------------------------
sub cntDF {
    my ($text,$type)=@_;

    $text=~s/<.+?>/ /gs;
    $text=~s/\s+/ /gs;

    # expand contractions
    $text=~s/I'm\b/I am/gi;
    $text=~s/\b(he|she|that|it)'s\b/$1 is/gi;
    $text=~s/'re\b/ are/gi;
    $text=~s/'ve\b/ have/gi;
    $text=~s/'ll\b/ will/gi;
    $text=~s/'d\b/ would/gi;

    # replace negations
    $text=~s/\b(cannot|can not|can't|could not|couldn't|will not|won't|would not|wouldn't|shall not|shan't|should not|shouldn't|must not|mustn't|does not|doesn't|do not|don't|did not|didn't|may not|might not|is not|isn't|am not|was not|wasn't|are not|aren't|were not|weren't|have not|haven't|had not|hadn't|need not|needn't)\b/ /gi;
    $text=~s/\b(hardly|hardly ever|never|barely|scarcely)\s+(can|could|will|would|shall|should|must|does|do|did|may|might|is|am|was|are|were|have to|had to|have|had|need)\b/ /gi;
    $text=~s/\b(can|could|will|would|shall|should|must|does|do|did|may|might|is|am|was|are|were|have to|had to|have|had|need)\s+(hardly|hardly ever|never|barely|scarcely|no)\b/ /gi;

    # compress select prepositions, conjunctions, articles
    $text=~s/\b(as|for|to|over|on|upon|in|with|of|and|but|or|a|an|the)\b/ /gi;
    $text=~s/\b(can|could|will|would|shall|should|must|do|did|may|might|am|was|are|were|have to|had to|have|had|need)\b/ /gi;
    $text=~s/\b(if|when|where|who|why|what|how|that|this|it|unless|because|then)\b/ /gi;

    $text=~s/\W/ /g;
    $text=~s/ +/ /g;

    # extract IU n-grams:
    #  - 'I|you|we|my|your|our WORD'
    #  - 'I|you|we|my|your|our WORD WORD'
    #  - 'WORD me|you|us'
    #  - 'WORD WORD me|you|us'
    my @ngram;
    while ($text=~/\b(I|you|we|my|your|our) ([\S]+)\b/gi) {
        push(@ngram,"$1 $2"); 
    }
    while ($text=~/\b(I|you|we|my|your|our) ([\S]+) ([\S]+)\b/gi) {
        push(@ngram,"$1 $2 $3");
    }       
    while ($text=~/\b([\S]+) (me|you|us)\b/gi) {
        push(@ngram,"$1 $2");
    }   
    while ($text=~/\b([\S]+) ([\S]+) (me|you|us)\b/gi) {
        push(@ngram,"$1 $2 $3");
    }   
            
    foreach my $ngram(@ngram) {
        $IU{lc($ngram)}{$type}++;
    }   

    # split words by space, hyphen, multiple comma/period
    $text=~s/[,\.]{2,}/ /g;
    my @wds= split/[\s\-]+/,lc($text);

    my %words;

    foreach my $wd(@wds) {

        # delete punctuations except !
        $wd=~s/[^\w!]+$//;
        $wd=~s/^[^\w]+//;

        # exclude null or single letter tokens
        next if ($wd=~/^\s*$/ || length($wd)<2 || $wd=~/[^a-zA-Z\-]/);

        $words{$wd}=1;
    }

    foreach my $wd(keys %words) {
        next if ($stoplist{$wd});
        $HF{$wd}{$type}++;
    }

} #endsub cntDF


#-------------------------------------------------
# ouput updated lexicon file
#-------------------------------------------------
#  arg1 = filename
#  arg2 = hptr to lexicon hash
#-------------------------------------------------
sub mkLex {
    my($outf,$lexhp,$outf2,$lexhp2)=@_;

    open(OUT,">$outf") || die "can't write to $outf";
    print LOG " - Writing to $outf\n";

    if ($outf2) {
        open(OUT2,">$outf2") || die "can't write to $outf2";
    }

    my %hash;
    foreach my $wd(keys %$lexhp) {
        $lexhp->{$wd}{'op'}=0 if (!$lexhp->{$wd}{'op'});
        $lexhp->{$wd}{'nop'}=0 if (!$lexhp->{$wd}{'nop'});
        my $op=$lexhp->{$wd}{'op'};
        my $nop=$lexhp->{$wd}{'nop'};
        my $p= (($op+1)/($opcnt+2))*(($nopcnt-$nop+1)/($nopcnt+2));
        $hash{$wd}=$p;
    }

    foreach my $wd(sort {$hash{$b}<=>$hash{$a}} keys %hash) {
        my $op=$lexhp->{$wd}{'op'};
        my $nop=$lexhp->{$wd}{'nop'};
        printf OUT "$wd %.4f $op $nop\n",$hash{$wd};
        printf OUT2 "$wd %.4f $op $nop\n",$hash{$wd} if ($lexhp2->{$wd});
    }

    close OUT;
    print LOG "\n";

}

    
#-------------------------------------------------
# create hashes of query text
#   - qdesc2, phrase, nr_phrase, nr_nouns should not be used for short query runs
#-------------------------------------------------
#  arg1 = processed query directory
#  arg2 = webx query phrases directory
#  arg3 = pointer to query title hash
#           k= QN, v= query title text (raw)
#  arg4 = pointer to processed query title hash
#           k= QN, v= query title text (stopped & stemmed)
#  arg5 = pointer to processed query description hash
#           k= QN, v= query description text (stopped & stemmed)
#  arg6 = pointer to processed query phrase hash
#           k= QN, v= hash pointer
#             k= phrase, v= freq
#  arg7 = pointer to expanded query phrase hash
#           k= QN, v= hash pointer
#             k= phrase, v= freq
#  arg8 = pointer to processed query non-relevant phrase hash
#           k= QN, v= hash pointer
#             k= nonrel_phrase, v= freq
#  arg9 = pointer to processed query non-relevant noun hash
#           k= QN, v= hash pointer
#             k= nonrel_noun, v= freq
#-------------------------------------------------
sub mkQRYhash {
    my($in,$in2,$hp)=@_;
            
    opendir(IND,$in) || die "can't opendir $in";
    my @files= readdir(IND);
    closedir IND;
        
    foreach my $file(@files) {
            
        next if ($file!~/^q(\d+)/);
        my $qn=$1;
                    
        # get expanded noun phrases
        my $inf2="$in2/qsx$qn";
        open(IN2,$inf2) || die "can't read $inf2";
        while(<IN2>) {
            chomp;
            my($wd,$wt)=split/ +/;
            $hp->{$qn}{lc($wd)}=1;
        }   
        close IN2;
            
        my $inf="$in/$file";
        open(IN,$inf) || die "can't read $inf";
        my @lines=<IN>;
        close IN;

        my $bstring= join("",@lines);

        # get title text
        if ($bstring=~m|<title>(.+?)</title>|s) {
            my $str=$1;
            $str=~m|<text>(.+?)</text>\n<text0>(.+?)</text0>|s;
            my ($ti,$ti2)=($1,$2);

            # delete quotations & punctuations
            $ti=~s/"//g;
            $ti=~s/^\W*(.+?)\W*$/$1/g;

            # accomodate (Boolean) OR in title text
            $ti=~s/\s*OR\s*/ /g if ($ti=~/^(.+?)\s+OR\b/);

            # accomodate changes in indexing module (7/20/2007)
            $ti2=$1 if ($ti2=~/^(.+?)\s+,/);

            foreach my $wd(split/ +/,"$ti $ti2") {
                $hp->{$qn}{lc($wd)}=1;
            }
        }

        # get description text
        if ($bstring=~m|<desc>(.+?)</desc>|s) {
            my $str=$1;
            $str=~m|<text0>(.+?)</text0>|s;
            my $desc=$1;

            # accomodate changes in indexing module (7/20/2007)
            $desc=$1 if ($desc=~/^(.+?)\s+,/);
            foreach my $wd(split/ +/,$desc) {
                $hp->{$qn}{lc($wd)}=1;
            }
        }

        # get noun phrases
        if (my @text= $bstring=~m|<phrase>(.+?)</phrase>|gs) {
            my %wds;
            foreach my $str(@text) {
                my @wds=split(/ +/,$str);
                foreach my $wd(@wds) {
                    $hp->{$qn}{lc($wd)}=1;
                }
            }
        }

    } #end-foreach $file(@files) 

} #endsub mkQRYhash

